model:
  class_name: model.LVSM_scene_encoder_decoder_ttt.Images2LatentScene

  image_tokenizer:
    image_size: 256
    patch_size: 8
    in_channels: 9  # 3 RGB + 3 direction + 3 Reference

  target_pose_tokenizer:
    image_size: 256
    patch_size: 8
    in_channels: 6  # 3 direction + 3 Reference
  transformer:
    d: 768
    d_head: 64
    encoder_n_layer: 12
    decoder_n_layer: 12
    special_init: true
    depth_init: true
    use_qk_norm: true
    n_latent_vectors: 3072 # 3x32x32
  ttt:
    n_layer: 1
    n_iters_per_layer: 12
    n_blocks_per_layer: 1
    n_blocks_per_layer_lrnet: 1
    state_lr: 1e-1
    state_lr_mode: adaptive_mlp
    state_lr_init: -2.0
    opt_model: transformer
    mlp_dim: 3072
    use_positional_encoding: true
    grad_mode: normal
    is_residual: true
    detach_s0: false
    detach_grad: true
    detach_decoder_input: false
    detach_opt_input: false
    detach_residual: false
    decode_chunk_size: 16 # test the best decoding chunk size on your setting
    supervise_mode: random_last
    min_layer: 0 # this is only used for random_last supervise mode
    max_layer: 12 # this is only used for random_last supervise mode
    distill_factor: 0.0
    normalizer_type: rms_norm
    normalizer_affine: true
    normalizer_eps: 1e-10
    n_encoder_inputs: 1
    adam:
      lr: 1e-3
      beta1: 0.9
      beta2: 0.999
      eps: 1e-8
      weight_decay: 0.0
    
training:
  # general
  num_threads: 8
  batch_size_per_gpu: 8
  train_steps: 100000
  
  # torch.compile settings
  use_torch_compile: false

  # dataset
  dataset_name: data.dataset_scene.Dataset
  dataset_path: ./preprocessed_data/train/full_list.txt
  scene_scale_factor: 1.35
  num_input_views: 2
  num_target_views: 6
  square_crop: true
  target_has_input: true # If True, target views can be the same as input views
  view_selector:
    max_frame_dist: 192
    min_frame_dist: 25

  # dataloader
  num_workers: 4
  prefetch_factor: 32

  # loss & grad
  supervision: target
  grad_accum_steps: 1
  grad_checkpoint: true
  grad_checkpoint_every: 1
  grad_clip_norm: 1.0
  allowed_gradnorm_factor: 5
  l2_loss_weight: 1.0
  lpips_loss_weight: 0.0
  perceptual_loss_weight: 0.5
  
  # optimizer & scheduler
  lr: 5e-5
  lr_ttt: 5e-5
  warmup: 3000
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  scheduler_type: constant
  
  # resume & checkpoint
  resume_ckpt: ""
  reset_training_state: false
  checkpoint_dir: ./experiments/checkpoints/LVSM_scene_encoder_decoder_ttt

  # amp & dtype
  use_amp: true
  amp_dtype: bf16
  use_tf32: true

  # wandb
  api_key_path: ./configs/api_keys.yaml
  wandb_exp_name: LVSM_scene_encoder_decoder_ttt
  wandb_project: LVSM_r2

  # logging
  print_every: 20
  wandb_log_every: 50
  vis_every: 1000
  checkpoint_every: 5000
  log_grad_norm_details: false

  # freeze parameters
  freeze_encoder: false
  freeze_decoder: false
  freeze_tokenizer: false
  freeze_latent: false


# inference / evaluation
inference:
  if_inference: False # whether to use testset or not.
  compute_metrics: True
  first_n_batches: None
  view_idx_file_path: ./data/evaluation_index_re10k.json
  render_video: False
  render_video_config:
    traj_type: interpolate
    num_frames: 60
    loop_video: True 
    order_poses: False
